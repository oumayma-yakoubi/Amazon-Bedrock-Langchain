{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b695dbf7-92d2-4ec8-8f3d-a2012f155533",
   "metadata": {},
   "source": [
    "## The Langchain integrations related to Amazon AWS platform : Amazon Bedrock "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50277a0e-a27a-4fc6-855b-7b2360e2af37",
   "metadata": {},
   "source": [
    "Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs)from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a signle API, along  with a broad set of capabilities you need to build generative AI applications with security, privacy and responsible AI. Using Amazon Bedrock, you can easly experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Agmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serveless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bfd891-ea3e-4b96-9db5-b7c0823b135d",
   "metadata": {},
   "source": [
    "# LLMs "
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ef83064-afac-40d3-806e-10093d6154e3",
   "metadata": {},
   "source": [
    "from lagchain_community.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(credentials_profile_name=\"bedrock-admin\", model_id=\"amazon.titan-text-express-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac9130-4204-49d3-bf94-22bb56dcb57a",
   "metadata": {},
   "source": [
    "#### Custom models "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a9368a7-a6f0-422f-b936-62267fd75a32",
   "metadata": {},
   "source": [
    "custom_llm= Bedrock(\n",
    "    credentials_profile_name=\"bedrock-admin\",\n",
    "    provider=\"cohere\"\n",
    "    model_id=\"<Custom model ARN>\", # ARN like 'arn:aws:bedrock:...' obtained via provisioning the custom model\n",
    "    model_kwargs={\"temperature\": 1},\n",
    "    streaming=True, \n",
    "    callbacks =[StreamingStdOutCallbackHandler]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206b7bd-e446-4f51-880d-6874f401a1c8",
   "metadata": {},
   "source": [
    "#### Using the LLM in a conversation chain "
   ]
  },
  {
   "cell_type": "raw",
   "id": "03e768ba-b0bf-4e7c-a37c-5ab5a5968c98",
   "metadata": {},
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "conversation = ConversationChain(llm = llm, verbose=True, memory= ConversationBufferMemory())\n",
    "\n",
    "conversation.predict(input= \"Hi there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286aab82-eaf8-431b-aba1-b1f6ba85727e",
   "metadata": {},
   "source": [
    "# Embedding Models "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd8ff234-550c-4e61-ab0d-e20cbbb4ae13",
   "metadata": {},
   "source": [
    "from langchain_community.embeddings import BedrockEmbeddings \n",
    "\n",
    "embeddings = BedrocEmbeddings( credentials_profile_name=\"bedrock-admin\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cd84683e-fea4-49ff-976d-972ba07e3c1b",
   "metadata": {},
   "source": [
    "embeddings.embed_query(\"This is a content of the document\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ae9550d-6689-4c12-ab75-d5b4af0f54e0",
   "metadata": {},
   "source": [
    "embeddings.embed_documents([\"This is a content of the document\", \"This is another document\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdea514a-cf22-497d-bd0a-92df71678b5b",
   "metadata": {},
   "source": [
    "# Amazon Bedrock (Knowledge Bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746b43ba-94b8-483b-a183-00d8357173c9",
   "metadata": {},
   "source": [
    "Knowledge bases for Amazon Bedrock is an Amazon Web Services (AWS) offering which lets you quickly build RAG applications by using your private data to customize foundation model response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2ec35b-d3d9-4035-ab55-95b36ed6a96a",
   "metadata": {},
   "source": [
    "Implementing RAG requires organization to perfom several cumbersome steps to convert data into embeddings(vectors), store the embeddings in a specialized vector database, and build custom integrations into the database and retrieve text relevant to the user's query. This can be time-consuming and inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5f871-51b9-4489-aab4-932d8adbafe2",
   "metadata": {},
   "source": [
    "With Knowledge Bases for Amazon Bedrock, simply point to the location of your data in Amazon S3, and knowledge bases for Amazon Bedrock takes care of the entire ingestion workflow into your vector database. If you do not have an existing vector database, Amazon Bedrock creates an Amazon OpenSearch Serverless vector store for you. For retrievals, use the Langchain-Amazon Bedrock integration via the Retrieve API to retrieve relevent results for a user query from knowledge bases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652762a-7e8c-4164-bb53-b8caeba577f8",
   "metadata": {},
   "source": [
    "## Using the Knowledge Bases Retriever"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81b806ae-b2fd-4ff4-889b-2aef52a106d4",
   "metadata": {},
   "source": [
    "from langchain_community.retrievers import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=\"PUIJP4EQUA\"\n",
    "    retrieval_config={\"vectorSearchConfiguration\" : {\"numberOfResults\": 4}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88f975-4b98-459b-bca7-073ddba9b514",
   "metadata": {},
   "source": [
    "#### Using the Knowledge Bases Retriever in a QA chain "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95945ee7-434f-40d0-b75f-148b867a202b",
   "metadata": {},
   "source": [
    "-- RestrievalQA : Chain of question-answering against an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d63e7f-b903-4534-8ff3-2076ad57614e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config \n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import Bedrock\n",
    "\n",
    "model_kwargs_claude = {\"temperature\": 0, \"top_k\": 10, \"max_tokens_to_sample\":3000}\n",
    "\n",
    "llm = Bedrock(model_id=\"anthropic.claude-v2\", model_kwargs=model_kwargs_claude)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever= retriever, return_source_documents=True)\n",
    "\n",
    "qa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
